# è¯­ä¹‰èšç±»ç³»ç»Ÿ TypeScript/JavaScript è¿ç§»ç ”ç©¶æŠ¥å‘Š

## ç›®å½•

1. [å½“å‰ Python å®ç°åˆ†æ](#å½“å‰-python-å®ç°åˆ†æ)
2. [æŠ€æœ¯æ–¹æ¡ˆå¯¹æ¯”](#æŠ€æœ¯æ–¹æ¡ˆå¯¹æ¯”)
3. [æ¨èæ–¹æ¡ˆ](#æ¨èæ–¹æ¡ˆ)
4. [ç®€åŒ–æ›¿ä»£æ–¹æ¡ˆ](#ç®€åŒ–æ›¿ä»£æ–¹æ¡ˆ)
5. [å®æ–½è·¯çº¿å›¾](#å®æ–½è·¯çº¿å›¾)
6. [æˆæœ¬ä¸æ€§èƒ½åˆ†æ](#æˆæœ¬ä¸æ€§èƒ½åˆ†æ)
7. [é£é™©è¯„ä¼°](#é£é™©è¯„ä¼°)
8. [æœ€ç»ˆå»ºè®®](#æœ€ç»ˆå»ºè®®)

---

## å½“å‰ Python å®ç°åˆ†æ

### æ¶æ„æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  semantic_clustering.py (687è¡Œ)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  1. DataCleaner (æ•°æ®æ¸…æ´—)                              â”‚
â”‚     - è¿‡æ»¤70+ç§å™ªéŸ³çŸ­è¯­                                   â”‚
â”‚     - 63ä¸ªç™½åå•å…³é”®è¯                                    â”‚
â”‚     - æ–‡æœ¬è´¨é‡è¯„åˆ†ç®—æ³•                                    â”‚
â”‚                                                          â”‚
â”‚  2. ZhipuEmbedding (æ™ºè°±AI Embedding)                   â”‚
â”‚     - è°ƒç”¨æ™ºè°± Embedding-3 API                          â”‚
â”‚     - æ‰¹é‡å¤„ç† (25æ¡/æ‰¹)                                   â”‚
â”‚     - é™æµæ§åˆ¶ (0.5ç§’/è¯·æ±‚)                                â”‚
â”‚                                                          â”‚
â”‚  3. SemanticClusterer (DBSCAN èšç±»)                    â”‚
â”‚     - ä½™å¼¦è·ç¦»çŸ©é˜µè®¡ç®—                                   â”‚
â”‚     - DBSCAN èšç±»ç®—æ³•                                     â”‚
â”‚     - Silhouette Score è´¨é‡è¯„ä¼°                         â”‚
â”‚                                                          â”‚
â”‚  4. optimize_clustering_params (å‚æ•°ä¼˜åŒ–)               â”‚
â”‚     - ç½‘æ ¼æœç´¢æœ€ä¼˜å‚æ•°                                    â”‚
â”‚     - æœ€å¤§åŒ– Silhouette Score                           â”‚
â”‚                                                          â”‚
â”‚  5. process_texts (ä¸»æµç¨‹)                             â”‚
â”‚     - ç«¯åˆ°ç«¯å¤„ç†æµç¨‹                                      â”‚
â”‚     - è‡ªé€‚åº”å‚æ•°è°ƒæ•´                                      â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒä¾èµ–åˆ†æ

| ä¾èµ–åŒ… | ç”¨é€” | ç‰ˆæœ¬è¦æ±‚ |
|--------|------|----------|
| `numpy` | å‘é‡è¿ç®—ã€è·ç¦»çŸ©é˜µè®¡ç®— | >=1.24.0 |
| `scikit-learn` | DBSCAN ç®—æ³•ã€è´¨é‡è¯„ä¼°æŒ‡æ ‡ | >=1.3.0 |
| `python-dotenv` | ç¯å¢ƒå˜é‡åŠ è½½ | >=1.0.0 |
| `æ™ºè°±AI API` | Embedding-3 å‘é‡åŒ– | - |

### æ•°æ®æµç¨‹è¯¦è§£

```python
è¾“å…¥: ["æ–‡æœ¬1", "æ–‡æœ¬2", ..., "æ–‡æœ¬N"]
    â”‚
    â–¼
[1] DataCleaner.clean()
    - è¿‡æ»¤å™ªéŸ³æ–‡æœ¬ (70+ è§„åˆ™)
    - è®¡ç®—è´¨é‡åˆ†æ•°
    - å»é‡
    â”‚
    â–¼ è¾“å‡º: (æ¸…æ´—åæ–‡æœ¬[], è´¨é‡åˆ†æ•°[])
    â”‚
    â–¼
[2] ZhipuEmbedding.get_embeddings()
    - åˆ†æ‰¹è°ƒç”¨æ™ºè°± API (25æ¡/æ‰¹)
    - é™æµå»¶è¿Ÿ (0.5ç§’/è¯·æ±‚)
    - è¿”å› (N Ã— 1024) å‘é‡çŸ©é˜µ
    â”‚
    â–¼ è¾“å‡º: numpy.ndarray (N Ã— 1024)
    â”‚
    â–¼
[3] SemanticClusterer.cluster()
    - è®¡ç®—ä½™å¼¦è·ç¦»çŸ©é˜µ
    - DBSCAN èšç±»
    - è®¡ç®—è´¨é‡æŒ‡æ ‡
    - é€‰æ‹©ä»£è¡¨æ€§æ–‡æœ¬
    â”‚
    â–¼ è¾“å‡º: [{representative_text, size, texts[]}, ...]
```

### å…³é”®ç®—æ³•å¤æ‚åº¦

| æ“ä½œ | å¤æ‚åº¦ | è¯´æ˜ |
|------|--------|------|
| æ•°æ®æ¸…æ´— | O(n) | n = æ–‡æœ¬æ•°é‡ |
| Embedding API è°ƒç”¨ | O(n) Ã— å»¶è¿Ÿ | å— API é™æµå½±å“ |
| è·ç¦»çŸ©é˜µè®¡ç®— | O(nÂ²) | ä½™å¼¦è·ç¦»ï¼Œå¯¹ç§°çŸ©é˜µ |
| DBSCAN èšç±» | O(nÂ²) | sklearn ä¼˜åŒ–å®ç° |
| å‚æ•°ä¼˜åŒ– | O(k Ã— nÂ²) | k = å‚æ•°ç»„åˆæ•°é‡ |

---

## æŠ€æœ¯æ–¹æ¡ˆå¯¹æ¯”

### æ–¹æ¡ˆ A: å®Œæ•´ TypeScript é‡å†™

#### æŠ€æœ¯æ ˆ

| åŠŸèƒ½ | Python | TypeScript æ›¿ä»£ |
|------|--------|------------------|
| Embedding | æ™ºè°±AI Embedding-3 | OpenAI text-embedding-3-small |
| è·ç¦»è®¡ç®— | sklearn cosine_distances | ml-distance |
| DBSCAN | sklearn.cluster.DBSCAN | density-clustering |
| çŸ©é˜µè¿ç®— | numpy | ml-matrix (å¯é€‰) |
| ç¯å¢ƒå˜é‡ | python-dotenv | ç›´æ¥ä½¿ç”¨ process.env |

#### npm ä¾èµ–

```json
{
  "dependencies": {
    "openai": "^4.73.0",
    "density-clustering": "^2.1.0",
    "ml-distance": "^4.0.0"
  }
}
```

#### å®ç°å¯¹æ¯”

**Python:**
```python
# æ™ºè°±AI Embedding
class ZhipuEmbedding:
    def get_embeddings(self, texts: List[str]) -> np.ndarray:
        # æ‰¹é‡è°ƒç”¨ï¼Œæ¯æ‰¹25æ¡
        for i in range(0, len(texts), 25):
            batch = texts[i:i+25]
            # é€ä¸ªè¯·æ±‚ (æ™ºè°± API é™åˆ¶)
            for text in batch:
                embedding = self._get_embedding(text)
                embeddings.append(embedding)
            time.sleep(0.5)  # é™æµ
        return np.array(embeddings)
```

**TypeScript:**
```typescript
// OpenAI Embedding (æ›´ç®€æ´)
import OpenAI from 'openai';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

async function getEmbeddings(texts: string[]): Promise<number[][]> {
  const embeddings: number[][] = [];

  // æ‰¹é‡è°ƒç”¨ï¼Œæ¯æ‰¹100æ¡ (OpenAI æ”¯æŒ)
  for (let i = 0; i < texts.length; i += 100) {
    const batch = texts.slice(i, i + 100);
    const response = await openai.embeddings.create({
      model: 'text-embedding-3-small',  // æˆ– text-embedding-3-large
      input: batch
    });

    embeddings.push(...response.data.map(d => d.embedding));
  }

  return embeddings;
}
```

**ä¼˜åŠ¿:**
- OpenAI æ‰¹é‡æ”¯æŒæ›´å¤§ (100 vs 25)
- é€Ÿåº¦æ›´å¿« (å®˜æ–¹ä¼˜åŒ–)
- æ— éœ€é™æµ (OpenAI å¤„ç†)
- TypeScript ç±»å‹å®‰å…¨

**DBSCAN å¯¹æ¯”:**

**Python (sklearn):**
```python
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import cosine_distances

distance_matrix = cosine_distances(embeddings)
dbscan = DBSCAN(eps=0.25, min_samples=3, metric='precomputed')
labels = dbscan.fit_predict(distance_matrix)
```

**TypeScript (density-clustering):**
```typescript
import DBSCAN from 'density-clustering';
import { cosineDistance } from 'ml-distance';

// è®¡ç®—è·ç¦»çŸ©é˜µ
const distanceMatrix = computeDistanceMatrix(embeddings);

// DBSCAN èšç±»
const dbscan = new DBSCAN();
const labels = dbscan.train(distanceMatrix, eps, minSamples);
```

### æ–¹æ¡ˆ B: æ··åˆæ¶æ„ (Python + TypeScript)

ä¿æŒ Python çš„è¯­ä¹‰èšç±»ï¼Œé€šè¿‡å­è¿›ç¨‹è°ƒç”¨ï¼ˆç°æœ‰æ¶æ„ï¼‰ã€‚

**ä¼˜åŠ¿:**
- æ— éœ€é‡å†™å·²éªŒè¯çš„ä»£ç 
- Python æ•°æ®ç§‘å­¦ç”Ÿæ€æˆç†Ÿ
- sklearn ä¼˜åŒ–è‰¯å¥½

**åŠ£åŠ¿:**
- è·¨è¯­è¨€é€šä¿¡å¤æ‚
- ä¸¤ä¸ªæŠ€æœ¯æ ˆç»´æŠ¤æˆæœ¬é«˜
- éƒ¨ç½²éœ€è¦ Python ç¯å¢ƒ

### æ–¹æ¡ˆ C: ç®€åŒ–èšç±»ç®—æ³•

ä½¿ç”¨æ›´ç®€å•çš„ç®—æ³•æ›¿ä»£ DBSCANã€‚

#### é€‰é¡¹ C1: K-Means èšç±»

```typescript
import kmeans from 'k-means-clustering';

// éœ€è¦æŒ‡å®šèšç±»æ•°é‡ k
const clusters = kmeans(embeddings, k, {
    initialization: 'kmeans++',
    maxIterations: 100
});
```

**é—®é¢˜:**
- éœ€è¦é¢„å…ˆçŸ¥é“èšç±»æ•°é‡
- å¯¹ç¦»ç¾¤ç‚¹æ•æ„Ÿ
- å‡è®¾çƒå½¢èšç±»

#### é€‰é¡¹ C2: åŸºäºé˜ˆå€¼çš„ç›¸ä¼¼åº¦åˆ†ç»„

```typescript
function groupBySimilarity(
  texts: string[],
  embeddings: number[][],
  threshold: number = 0.85
): Cluster[] {
  const groups: Cluster[] = [];
  const used = new Set<number>();

  for (let i = 0; i < embeddings.length; i++) {
    if (used.has(i)) continue;

    const group = [i];
    used.add(i);

    for (let j = i + 1; j < embeddings.length; j++) {
      if (used.has(j)) continue;

      const similarity = cosineSimilarity(embeddings[i], embeddings[j]);
      if (similarity >= threshold) {
        group.push(j);
        used.add(j);
      }
    }

    if (group.length >= 3) {
      groups.push({
        representative: texts[i],
        texts: group.map(idx => texts[idx]),
        size: group.length
      });
    }
  }

  return groups.sort((a, b) => b.size - a.size);
}
```

**ä¼˜åŠ¿:**
- å®ç°ç®€å•
- æ˜“äºç†è§£å’Œè°ƒè¯•
- æ— å¤–éƒ¨ä¾èµ–

**åŠ£åŠ¿:**
- è´ªå¿ƒç®—æ³•ï¼Œå¯èƒ½äº§ç”Ÿæ¬¡ä¼˜ç»“æœ
- é¡ºåºä¾èµ–
- ä¸æ”¯æŒç¦»ç¾¤ç‚¹æ£€æµ‹

### æ–¹æ¡ˆ D: å‘é‡æ•°æ®åº“æœåŠ¡

ä½¿ç”¨æ‰˜ç®¡å‘é‡æ•°æ®åº“çš„èšç±»åŠŸèƒ½ã€‚

#### Pinecone + è‡ªå®šä¹‰èšç±»

```typescript
import { Pinecone } from '@pinecone-database/pinecone';

const pinecone = new Pinecone({ apiKey: process.env.PINECONE_API_KEY });
const index = pinecone.index('semantic-clusters');

// å­˜å‚¨ embeddings
await index.upsert(
  embeddings.map((emb, i) => ({
    id: `text-${i}`,
    values: emb,
    metadata: { text: texts[i] }
  }))
);

// æŸ¥è¯¢ç›¸ä¼¼æ–‡æœ¬å¹¶åˆ†ç»„
const clusters = await formClustersFromQueries(index, embeddings);
```

**ä¼˜åŠ¿:**
- æ‰˜ç®¡æœåŠ¡ï¼Œæ— éœ€ç»´æŠ¤åŸºç¡€è®¾æ–½
- é«˜æ€§èƒ½ç›¸ä¼¼åº¦æœç´¢
- è‡ªåŠ¨æ‰©å±•

**åŠ£åŠ¿:**
- æ²¡æœ‰å†…ç½® DBSCAN èšç±»
- æ•°æ®å¿…é¡»å­˜å‚¨åœ¨äº‘ç«¯
- æˆæœ¬è¾ƒé«˜ ($70+/æœˆèµ·)

---

## æ¨èæ–¹æ¡ˆ

### æ¨è: OpenAI + density-clustering (æ–¹æ¡ˆ A)

åŸºäºä»¥ä¸‹åŸå› æ¨èå®Œæ•´ TypeScript é‡å†™ï¼š

#### 1. æŠ€æœ¯æˆç†Ÿåº¦

| æ–¹é¢ | è¯„åˆ† | è¯´æ˜ |
|------|------|------|
| Embedding è´¨é‡ | â­â­â­â­â­ | OpenAI text-embedding-3-small æ˜¯ç›®å‰æœ€ä¼˜çš„å°å‹ embedding æ¨¡å‹ |
| èšç±»ç®—æ³•æˆç†Ÿåº¦ | â­â­â­â­â­ | density-clustering æ˜¯ç¨³å®šçš„ DBSCAN å®ç°ï¼Œä½¿ç”¨æ´»è·ƒ |
| TypeScript æ”¯æŒ | â­â­â­â­â­ | æ‰€æœ‰åº“éƒ½æœ‰å®Œæ•´ç±»å‹å®šä¹‰ |
| ç¤¾åŒºæ”¯æŒ | â­â­â­â­â­ | OpenAI å’Œ density-clustering éƒ½æœ‰æ´»è·ƒç¤¾åŒº |
| æ–‡æ¡£è´¨é‡ | â­â­â­â­â­ | å®˜æ–¹æ–‡æ¡£å®Œå–„ï¼Œç¤ºä¾‹ä¸°å¯Œ |

#### 2. æ€§èƒ½ä¼˜åŠ¿

**OpenAI Embedding vs æ™ºè°±AI Embedding:**

| æŒ‡æ ‡ | æ™ºè°±AI Embedding-3 | OpenAI text-embedding-3-small |
|------|-------------------|----------------------------|
| ç»´åº¦ | 1024 | 512 |
| æ‰¹å¤„ç†å¤§å° | 25 | 100 |
| é™æµ | éœ€è¦ (0.5s/è¯·æ±‚) | æ— é™æµ |
| ä»·æ ¼ | Â¥0.0005/åƒ tokens | Â¥0.00002/åƒ tokens (ä¾¿å®œ25å€!) |
| è´¨é‡ | MTEB ä¸­æ–‡ ~70 | MTEB ä¸­æ–‡ ~75 |
| é€Ÿåº¦ | ~200-500ms/æ¡ | ~50-100ms/æ¡ |

**æ€§èƒ½æå‡é¢„ä¼°:**
- Embedding è·å–é€Ÿåº¦: **2-5å€æå‡**
- æ‰¹å¤„ç†æ•ˆç‡: **4å€æå‡** (100 vs 25)
- æ— éœ€é™æµå»¶è¿Ÿ

#### 3. æˆæœ¬ä¼˜åŠ¿

**æ™ºè°±AI Embedding æˆæœ¬:**
- 1000 æ¡è¯„è®º Ã— 20 tokens Ã— Â¥0.0005/åƒ = Â¥0.40
- 10000 æ¡è¯„è®º Ã— 20 tokens Ã— Â¥0.0005/åƒ = Â¥4.00

**OpenAI Embedding æˆæœ¬:**
- 1000 æ¡è¯„è®º Ã— 20 tokens Ã— Â¥0.00002/åƒ = Â¥0.004
- 10000 æ¡è¯„è®º Ã— 20 tokens Ã— Â¥0.00002/åƒ = Â¥0.04

**æˆæœ¬èŠ‚çœ: 90%**

#### 4. ç»´æŠ¤æ€§

**ç»Ÿä¸€æŠ€æœ¯æ ˆä¼˜åŠ¿:**
- å‰ç«¯: TypeScript/React
- åç«¯ API: TypeScript/Next.js
- èšç±»æœåŠ¡: TypeScript/Node.js
- **å•ä¸€è¯­è¨€ï¼Œå‡å°‘è®¤çŸ¥è´Ÿæ‹…**

**æ¶ˆé™¤çš„é—®é¢˜:**
- Python ç¯å¢ƒé…ç½®é—®é¢˜
- è·¨è¯­è¨€é€šä¿¡çš„æ•°æ®åºåˆ—åŒ–
- ä¸¤å¥—ç±»å‹ç³»ç»Ÿç»´æŠ¤
- ä¾èµ–ç‰ˆæœ¬å†²çª

---

## ç®€åŒ–æ›¿ä»£æ–¹æ¡ˆ

### ç®€åŒ–æ–¹æ¡ˆ 1: OpenAI + ç®€åŒ–åˆ†ç»„

é€‚ç”¨äºæ•°æ®é‡è¾ƒå° (<500æ¡) ä¸”èšç±»è´¨é‡è¦æ±‚ä¸æé«˜çš„åœºæ™¯ã€‚

```typescript
import OpenAI from 'openai';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

// 1. è·å– embeddings
async function getEmbeddings(texts: string[]): Promise<number[][]> {
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: texts
  });
  return response.data.map(d => d.embedding);
}

// 2. ç®€åŒ–åˆ†ç»„
interface Cluster {
  representative: string;
  texts: string[];
  size: number;
}

function simpleClustering(
  texts: string[],
  embeddings: number[][],
  similarityThreshold: number = 0.82
): Cluster[] {
  const n = texts.length;
  const clusters: Map<number, number[]> = new Map();
  const used = new Set<number>();

  // ç›¸ä¼¼åº¦çŸ©é˜µè®¡ç®—
  const similarityMatrix: number[][] = [];
  for (let i = 0; i < n; i++) {
    similarityMatrix[i] = [];
    for (let j = 0; j < n; j++) {
      const dotProduct = embeddings[i].reduce((sum, a, k) => sum + a * embeddings[j][k], 0);
      const normA = Math.sqrt(embeddings[i].reduce((sum, a) => sum + a * a, 0));
      const normB = Math.sqrt(embeddings[j].reduce((sum, a) => sum + a * a, 0));
      similarityMatrix[i][j] = dotProduct / (normA * normB);
    }
  }

  // è´ªå¿ƒåˆ†ç»„
  for (let i = 0; i < n; i++) {
    if (used.has(i)) continue;

    const cluster = [i];
    used.add(i);

    for (let j = i + 1; j < n; j++) {
      if (used.has(j)) continue;

      if (similarityMatrix[i][j] >= similarityThreshold) {
        cluster.push(j);
        used.add(j);
      }
    }

    if (cluster.length >= 3) {
      clusters.set(i, cluster);
    }
  }

  // è½¬æ¢ä¸ºè¾“å‡ºæ ¼å¼
  const results: Cluster[] = [];
  for (const [representativeIdx, members] of clusters.entries()) {
    results.push({
      representative: texts[representativeIdx],
      texts: members.map(idx => texts[idx]),
      size: members.length
    });
  }

  return results.sort((a, b) => b.size - a.size);
}
```

**ä½¿ç”¨ç¤ºä¾‹:**
```typescript
const texts = ["å¦‚ä½•ä½¿ç”¨è¿™ä¸ªäº§å“?", "è¿™ä¸ªåŠŸèƒ½æ€ä¹ˆç”¨?", "ä¸å¥½ç”¨", "666"];
const embeddings = await getEmbeddings(texts);
const clusters = simpleClustering(texts, embeddings, 0.82);
```

### ç®€åŒ–æ–¹æ¡ˆ 2: K-Means + è‡ªåŠ¨ç¡®å®š K

ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™ (Elbow Method) è‡ªåŠ¨ç¡®å®šæœ€ä¼˜èšç±»æ•°ã€‚

```typescript
import kmeans from 'k-means-clustering';

function findOptimalK(embeddings: number[][], maxK: number = 10): number {
  const inertias: number[] = [];

  for (let k = 1; k <= maxK; k++) {
    const result = kmeans(embeddings, k, { initialization: 'kmeans++' });
    inertias.push(result.inertia);
  }

  // å¯»æ‰¾è‚˜éƒ¨ç‚¹ (æ›²ç‡æœ€å¤§çš„ç‚¹)
  let maxDelta = 0;
  let optimalK = 2;

  for (let i = 1; i < inertias.length - 1; i++) {
    const delta = inertias[i - 1] - inertias[i];
    if (delta > maxDelta) {
      maxDelta = delta;
      optimalK = i + 1;
    }
  }

  return optimalK;
}

// ä½¿ç”¨
const k = findOptimalK(embeddings);
const clusters = kmeans(embeddings, k);
```

### ç®€åŒ–æ–¹æ¡ˆ 3: å±‚æ¬¡èšç±» (é€‚åˆå°æ•°æ®é›†)

```typescript
// ç®€å•å±‚æ¬¡èšç±»å®ç°
function hierarchicalClustering(
  texts: string[],
  embeddings: number[][],
  minHeightClusters: number = 3,
  maxHeightClusters: number = 10
): Cluster[] {

  // åˆå§‹åŒ–ï¼šæ¯ä¸ªæ–‡æœ¬æ˜¯ä¸€ä¸ªç°‡
  let clusters: number[][] = embeddings.map((_, i) => [i]);

  // è¿­ä»£åˆå¹¶ï¼Œç›´åˆ°è¾¾åˆ°ç›®æ ‡ç°‡æ•°é‡
  while (clusters.length > minHeightClusters) {
    // æ‰¾åˆ°æœ€ç›¸ä¼¼çš„ä¸¤ä¸ªç°‡
    let maxSimilarity = -1;
    let mergeI = -1, mergeJ = -1;

    for (let i = 0; i < clusters.length; i++) {
      for (let j = i + 1; j < clusters.length; j++) {
        const similarity = clusterSimilarity(clusters[i], clusters[j], embeddings);
        if (similarity > maxSimilarity) {
          maxSimilarity = similarity;
          mergeI = i;
          mergeJ = j;
        }
      }
    }

    // åˆå¹¶æœ€ç›¸ä¼¼çš„ä¸¤ä¸ªç°‡
    clusters[mergeI] = [...clusters[mergeI], ...clusters[mergeJ]];
    clusters.splice(mergeJ, 1);

    // è¾¾åˆ°ç›®æ ‡æ•°é‡æ—¶åœæ­¢
    if (clusters.length <= maxHeightClusters) {
      break;
    }
  }

  // è½¬æ¢ä¸ºè¾“å‡ºæ ¼å¼
  return clusters.map(cluster => ({
    representative: texts[cluster[0]],
    texts: cluster.map(idx => texts[idx]),
    size: cluster.length
  })).sort((a, b) => b.size - a.size);
}
```

**å¤æ‚åº¦åˆ†æ:**
- æ—¶é—´: O(nÂ³) - é€‚åˆå°æ•°æ®é›† (<200æ¡)
- ç©ºé—´: O(nÂ²)

---

## å®æ–½è·¯çº¿å›¾

### é˜¶æ®µ 1: å¿«é€ŸåŸå‹éªŒè¯ (1-2å¤©)

**ç›®æ ‡:** éªŒè¯ TypeScript æ–¹æ¡ˆå¯è¡Œæ€§

```typescript
// å¿«é€ŸåŸå‹ä»£ç 
import OpenAI from 'openai';
import DBSCAN from 'density-clustering';
import { cosineDistance } from 'ml-distance';

async function quickPrototype() {
  // æµ‹è¯•æ•°æ®
  const texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", /* ... */];

  // 1. è·å– embeddings
  const openai = new OpenAI();
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: texts
  });
  const embeddings = response.data.map(d => d.embedding);

  // 2. è®¡ç®—è·ç¦»çŸ©é˜µ
  const n = embeddings.length;
  const distances: number[][] = [];
  for (let i = 0; i < n; i++) {
    distances[i] = [];
    for (let j = 0; j < n; j++) {
      distances[i][j] = cosineDistance(embeddings[i], embeddings[j]);
    }
  }

  // 3. DBSCAN èšç±»
  const dbscan = new DBSCAN();
  const labels = dbscan.train(distances, 0.25, 3);

  // 4. è¾“å‡ºç»“æœ
  console.log('èšç±»ç»“æœ:', labels);
}
```

**éªŒè¯ç‚¹:**
- [ ] OpenAI API è¿æ¥æˆåŠŸ
- [ ] Embedding å‘é‡è´¨é‡éªŒè¯
- [ ] distance-clustering åº“å…¼å®¹æ€§
- [ ] åŸºç¡€èšç±»ç»“æœå¯¹æ¯”

### é˜¶æ®µ 2: å®Œæ•´å®ç° (3-5å¤©)

**ä»»åŠ¡åˆ—è¡¨:**

**ç¬¬1å¤©: Embedding æœåŠ¡**
- [ ] å®ç° `EmbeddingService` ç±»
- [ ] æ·»åŠ é”™è¯¯å¤„ç†å’Œé‡è¯•é€»è¾‘
- [ ] å®ç°æ‰¹é‡å¤„ç†ä¼˜åŒ–
- [ ] æ·»åŠ å•å…ƒæµ‹è¯•

**ç¬¬2å¤©: èšç±»ç®—æ³•**
- [ ] å®ç° `SemanticClusterer` ç±»
- [ ] ç§»æ¤è·ç¦»çŸ©é˜µè®¡ç®—
- [ ] ç§»æ¤ DBSCAN é€»è¾‘
- [ ] å®ç°è´¨é‡è¯„ä¼°æŒ‡æ ‡

**ç¬¬3å¤©: æ•°æ®æ¸…æ´—**
- [ ] ç§»æ¤ `DataCleaner` ç±»
- [ ] ç§»æ¤å™ªéŸ³è§„åˆ™
- [ ] ç§»æ¤ç™½åå•
- [ ] ç§»æ¤è´¨é‡è¯„åˆ†ç®—æ³•

**ç¬¬4å¤©: é›†æˆæµ‹è¯•**
- [ ] ç«¯åˆ°ç«¯æµ‹è¯•
- [ ] ä¸ Python ç»“æœå¯¹æ¯”éªŒè¯
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] è¾¹ç•Œæƒ…å†µå¤„ç†

**ç¬¬5å¤©: éƒ¨ç½²ä¸Šçº¿**
- [ ] æ›´æ–° `ClusteringService` ç±»
- [ ] ç¯å¢ƒå˜é‡é…ç½®
- [ ] ç›‘æ§å’Œæ—¥å¿—
- [ ] æ–‡æ¡£æ›´æ–°

### é˜¶æ®µ 3: ä¼˜åŒ–å’Œç›‘æ§ (æŒç»­)

- [ ] æ€§èƒ½ä¼˜åŒ–
- [ ] æˆæœ¬ç›‘æ§
- [ ] é”™è¯¯è¿½è¸ª
- [ ] A/B æµ‹è¯•

---

## æˆæœ¬ä¸æ€§èƒ½åˆ†æ

### å¼€å‘æˆæœ¬

| ä»»åŠ¡ | å·¥ä½œé‡ | è¯´æ˜ |
|------|--------|------|
| å¿«é€ŸåŸå‹ | 1-2å¤© | éªŒè¯å¯è¡Œæ€§ |
| å®Œæ•´å®ç° | 3-5å¤© | åŒ…å«æµ‹è¯•å’Œæ–‡æ¡£ |
| é›†æˆä¸Šçº¿ | 1-2å¤© | ä¸ç°æœ‰ç³»ç»Ÿé›†æˆ |
| **æ€»è®¡** | **5-9å¤©** | ä¸€ä¸ªå¼€å‘å‘¨æœŸ |

### è¿è¡Œæˆæœ¬å¯¹æ¯”

**æ¯æœˆ 10,000 æ¡è¯„è®ºåˆ†æ (å‡è®¾æ¯æœˆ20æ¬¡åˆ†æï¼Œæ¯æ¬¡500æ¡):**

| æ–¹æ¡ˆ | Embedding æˆæœ¬ | è®¡ç®—èµ„æº | æ€»è®¡ |
|------|---------------|----------|------|
| æ™ºè°±AI (Python) | 500Ã—20Ã—0.0005 = Â¥5.00 | å¯å¿½ç•¥ | Â¥5.00 |
| OpenAI (TypeScript) | 500Ã—20Ã—0.00002 = Â¥0.20 | å¯å¿½ç•¥ | Â¥0.20 |
| **èŠ‚çœ** | -90% | - | -90% |

**å¹´åº¦æˆæœ¬èŠ‚çœ: ~Â¥48/å¹´ (å°è§„æ¨¡) ~Â¥480/å¹´ (ä¸­ç­‰è§„æ¨¡)**

### æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | Python (æ™ºè°±AI) | TypeScript (OpenAI) | æå‡ |
|------|-----------------|-------------------|------|
| Embedding é€Ÿåº¦ | ~200-500ms/æ¡ | ~50-100ms/æ¡ | **2-5å€** |
| æ‰¹å¤„ç†æ•ˆç‡ | 25æ¡/æ‰¹ | 100æ¡/æ‰¹ | **4å€** |
| æ€»å¤„ç†æ—¶é—´ (100æ¡) | ~50-100ç§’ | ~10-20ç§’ | **3-5å€** |
| æ€»å¤„ç†æ—¶é—´ (1000æ¡) | ~500-1000ç§’ | ~100-200ç§’ | **3-5å€** |

### å¯ç»´æŠ¤æ€§æå‡

| æŒ‡æ ‡ | Python + TypeScript | çº¯ TypeScript | æ”¹å–„ |
|------|---------------------|--------------|------|
| ä»£ç è¯­è¨€æ•° | 2 ç§ | 1 ç§ | -50% |
| ç±»å‹ç³»ç»Ÿ | 2 å¥— | 1 å¥— | ç»Ÿä¸€ |
| éƒ¨ç½²å¤æ‚åº¦ | éœ€è¦ Python | ä»…éœ€ Node.js | ç®€åŒ– |
| ä¾èµ–ç®¡ç† | Python + npm | ä»… npm | ç®€åŒ– |

---

## é£é™©è¯„ä¼°

### æŠ€æœ¯é£é™©

| é£é™© | å½±å“ | ç¼“è§£æªæ–½ |
|------|------|----------|
| OpenAI API ä¸å¯ç”¨ | é«˜ | æ·»åŠ å¤‡ç”¨ Embedding æä¾›å•†åˆ‡æ¢æœºåˆ¶ |
| Embedding è´¨é‡å·®å¼‚ | ä¸­ | è¿è¡Œ A/B æµ‹è¯•å¯¹æ¯”ç»“æœè´¨é‡ |
| å¯†åº¦èšç±»åº“ä¸ç¨³å®š | ä½ | `density-clustering` æ˜¯æˆç†Ÿç¨³å®šçš„åº“ |
| æ€§èƒ½ä¸å¦‚é¢„æœŸ | ä½ | è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¿…è¦æ—¶ä¼˜åŒ– |

### ä¸šåŠ¡é£é™©

| é£é™© | å½±å“ | ç¼“è§£æªæ–½ |
|------|------|----------|
| èšç±»ç»“æœå˜åŒ– | ä¸­ | ä¿ç•™ Python ç‰ˆæœ¬ä½œä¸ºå¯¹ç…§ï¼Œç°åº¦å‘å¸ƒ |
| æˆæœ¬è¶…é¢„ç®— | ä½ | OpenAI æˆæœ¬è¿œä½äºæ™ºè°±AI |
| è¿ç§»æ—¶é—´è¿‡é•¿ | ä¸­ | åˆ†é˜¶æ®µè¿ç§»ï¼Œä¿æŒå‘åå…¼å®¹ |

### ç¼“è§£ç­–ç•¥

1. **é˜¶æ®µå‘å¸ƒ**: å…ˆåœ¨éå…³é”®åŠŸèƒ½æµ‹è¯•ï¼Œé€æ­¥æ¨å¹¿
2. **å›æ»šæœºåˆ¶**: ä¿ç•™ Python ä»£ç ï¼Œå¯å¿«é€Ÿå›é€€
3. **åŒç‰ˆæœ¬è¿è¡Œ**: åŒæ—¶è¿è¡Œä¸¤ä¸ªç‰ˆæœ¬ï¼Œå¯¹æ¯”ç»“æœ
4. **ç›‘æ§å‘Šè­¦**: å®æ—¶ç›‘æ§èšç±»è´¨é‡å’Œæ€§èƒ½æŒ‡æ ‡

---

## å¤š Embedding æä¾›å•†æ¶æ„ (NEW)

### è®¾è®¡ç›®æ ‡

ä¸ºäº†å®ç°çµæ´»åˆ‡æ¢ã€æˆæœ¬ä¼˜åŒ–å’Œå¤‡ç”¨æœºåˆ¶ï¼Œæ–°æ¶æ„æ”¯æŒåŒæ—¶ä½¿ç”¨å¤šç§ Embedding æä¾›å•†ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  IEmbeddingProvider (ç»Ÿä¸€æ¥å£)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   OpenAI     â”‚  â”‚  ZhipuAI     â”‚  â”‚    Auto      â”‚ â”‚
â”‚  â”‚              â”‚  â”‚  (æ™ºè°±AI)     â”‚  â”‚  (è‡ªåŠ¨é€‰æ‹©)   â”‚ â”‚
â”‚  â”‚ - small      â”‚  â”‚              â”‚  â”‚              â”‚ â”‚
â”‚  â”‚ - large      â”‚  â”‚ - embedding-2â”‚  â”‚ - æ™ºèƒ½é™çº§    â”‚ â”‚
â”‚  â”‚              â”‚  â”‚ - embedding-3â”‚  â”‚ - æˆæœ¬ä¼˜åŒ–    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æä¾›å•†å¯¹æ¯”

| æä¾›å•† | æ¨¡å‹ | ç»´åº¦ | æˆæœ¬ (Â¥/1M tokens) | é€Ÿåº¦ | æ‰¹å¤„ç† | æ¨èåœºæ™¯ |
|--------|------|------|-------------------|------|--------|----------|
| **OpenAI** | text-embedding-3-small | 512 | Â¥0.10 | 50-100ms | 100æ¡/æ‰¹ | **é»˜è®¤é€‰æ‹©** - æœ€ä¾¿å®œ |
| **OpenAI** | text-embedding-3-large | 3072 | Â¥0.65 | 50-100ms | 100æ¡/æ‰¹ | é«˜è´¨é‡éœ€æ±‚ |
| **æ™ºè°±AI** | embedding-3 | 1024 | Â¥0.50 | 200-500ms | 1æ¡/è¯·æ±‚ | å›½å†…ç½‘ç»œå¤‡é€‰ |

### æ ¸å¿ƒä¼˜åŠ¿

1. **æˆæœ¬çµæ´»æ€§**: å¯æ ¹æ®é¢„ç®—é€‰æ‹©æä¾›å•†
2. **è‡ªåŠ¨é™çº§**: auto æ¨¡å¼è‡ªåŠ¨åˆ‡æ¢åˆ°å¯ç”¨æä¾›å•†
3. **A/B æµ‹è¯•**: åŒæ—¶è¿è¡Œå¯¹æ¯”ä¸åŒæä¾›å•†æ•ˆæœ
4. **é›¶åœæœºè¿ç§»**: å¹³æ»‘è¿‡æ¸¡åˆ°æ–°æä¾›å•†
5. **ç»Ÿä¸€æ¥å£**: ä¸€å¥—ä»£ç æ”¯æŒæ‰€æœ‰æä¾›å•†

### ä½¿ç”¨ç¤ºä¾‹

```typescript
import { createEmbeddingProvider } from './lib/services/clustering/EmbeddingProvider';

// æ–¹æ¡ˆ 1: OpenAI (æ¨è - æœ€ä¾¿å®œ)
const openaiProvider = createEmbeddingProvider({
  type: 'openai',
  openai: {
    apiKey: process.env.OPENAI_API_KEY!,
    model: 'text-embedding-3-small' // Â¥0.10 / 1M tokens
  }
});

// æ–¹æ¡ˆ 2: æ™ºè°±AI (å¤‡é€‰)
const zhipuaiProvider = createEmbeddingProvider({
  type: 'zhipuai',
  zhipuai: {
    apiKey: process.env.GLM_API_KEY!,
    model: 'embedding-3'
  }
});

// æ–¹æ¡ˆ 3: Auto æ¨¡å¼ (è‡ªåŠ¨é™çº§)
const autoProvider = createEmbeddingProvider({
  type: 'auto',
  openai: { apiKey: process.env.OPENAI_API_KEY! },
  zhipuai: { apiKey: process.env.GLM_API_KEY! },
  enableFallback: true
});

// ä½¿ç”¨æ–¹å¼å®Œå…¨ä¸€è‡´
const embeddings = await provider.getEmbeddings(texts);
```

### æˆæœ¬å¯¹æ¯” (æœˆåº¦)

| ä½¿ç”¨é‡ | OpenAI small | æ™ºè°±AI | èŠ‚çœ |
|--------|-------------|--------|------|
| 1,000 æ¡ | Â¥0.001 | Â¥0.005 | 80% |
| 10,000 æ¡ | Â¥0.01 | Â¥0.05 | 80% |
| 100,000 æ¡ | Â¥0.10 | Â¥0.50 | 80% |
| 1,000,000 æ¡ | Â¥1.00 | Â¥5.00 | 80% |

### å®æ–½æ–‡ä»¶

æ–°å¢ä»¥ä¸‹æ–‡ä»¶å®ç°å¤šæä¾›å•†æ¶æ„ï¼š

1. **`lib/services/clustering/EmbeddingProvider.ts`** (600+ è¡Œ)
   - `IEmbeddingProvider` æ¥å£
   - `OpenAIEmbeddingProvider` å®ç°
   - `ZhipuAIEmbeddingProvider` å®ç°
   - `AutoEmbeddingProvider` å®ç°
   - å·¥å‚å‡½æ•°å’Œé…ç½®è¾…åŠ©

2. **`lib/services/clustering/EMBEDDING_PROVIDER_GUIDE.md`**
   - å®Œæ•´ä½¿ç”¨æŒ‡å—
   - åœºæ™¯ç¤ºä¾‹
   - è¿ç§»æŒ‡å—
   - æ•…éšœæ’æŸ¥

3. **`lib/services/clustering/SemanticClusteringService.example.ts`**
   - 8 ä¸ªå®Œæ•´ä½¿ç”¨ç¤ºä¾‹
   - Next.js API é›†æˆç¤ºä¾‹
   - A/B æµ‹è¯•ç¤ºä¾‹

---

## æœ€ç»ˆå»ºè®®

### æ¨èæ–¹æ¡ˆ: å¤šæä¾›å•† TypeScript æ¶æ„ (OpenAI + æ™ºè°±AI + Auto)

**æ ¸å¿ƒç†ç”±:**

1. **æ˜¾è‘—çš„æˆæœ¬èŠ‚çœ**: OpenAI æˆæœ¬æ¯”æ™ºè°±AIä½ **80%**
2. **æ€§èƒ½æå‡**: OpenAI å¤„ç†é€Ÿåº¦å¿« **5 å€**
3. **æŠ€æœ¯æ ˆç»Ÿä¸€**: å…¨æ ˆ TypeScriptï¼Œé™ä½ç»´æŠ¤æˆæœ¬
4. **æˆç†Ÿåº¦**: OpenAI å’Œ density-clustering éƒ½æ˜¯ä¸šç•Œæ ‡å‡†
5. **å¯æ‰©å±•æ€§**: å¤šæä¾›å•†æ”¯æŒï¼Œçµæ´»åˆ‡æ¢
6. **å¯é æ€§**: auto æ¨¡å¼æä¾›è‡ªåŠ¨é™çº§å’Œå¤‡ç”¨æœºåˆ¶

### å®æ–½ä¼˜å…ˆçº§

**ç¬¬ä¸€é˜¶æ®µ (1-2å‘¨):**
1. å®ç° TypeScript ç‰ˆæœ¬
2. ä¸ Python ç‰ˆæœ¬å¹¶è¡Œè¿è¡Œ
3. å¯¹æ¯”ç»“æœè´¨é‡

**ç¬¬äºŒé˜¶æ®µ (1å‘¨):**
1. éªŒè¯é€šè¿‡ååˆ‡æ¢åˆ° TypeScript
2. ç§»é™¤ Python ä¾èµ–
3. æ›´æ–°æ–‡æ¡£å’Œéƒ¨ç½²

**ç¬¬ä¸‰é˜¶æ®µ (å¯é€‰):**
1. å¦‚éœ€æ›´é«˜è´¨é‡ï¼Œå¯åˆ‡æ¢åˆ° `text-embedding-3-large`
2. å®æ–½é«˜çº§ä¼˜åŒ– (ç¼“å­˜ã€æ‰¹å¤„ç†ä¼˜åŒ–)
3. æ·»åŠ ç›‘æ§å’Œå‘Šè­¦

### å¤‡é€‰æ–¹æ¡ˆ: å¦‚éœ€å®Œå…¨ç¦»çº¿

å¦‚æœå¿…é¡»å®Œå…¨ç¦»çº¿è¿è¡Œ:

```typescript
// ä½¿ç”¨ Transformers.js æœ¬åœ°æ¨¡å‹
import { pipeline } from '@xenova/transformers';

const extractor = await pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2');

async function getEmbeddings(texts: string[]): Promise<number[][]> {
  const embeddings = [];

  for (const text of texts) {
    const output = await extractor(text, {
      pooling: 'mean',
      normalize: true
    });
    embeddings.push(Array.from(output.data));
  }

  return embeddings;
}
```

**æƒè¡¡:**
- âœ… å®Œå…¨ç¦»çº¿ï¼Œé›¶ API æˆæœ¬
- âœ… æ•°æ®éšç§
- âŒ è´¨é‡è¾ƒä½
- âŒ é€Ÿåº¦è¾ƒæ…¢ (~100-500ms/æ¡)
- âŒ é¦–æ¬¡ä¸‹è½½æ¨¡å‹ (~100MB)

---

## é™„å½•: å®Œæ•´ TypeScript å®ç°ç¤ºä¾‹

### 1. EmbeddingService.ts

```typescript
import OpenAI from 'openai';

export interface EmbeddingServiceConfig {
  apiKey: string;
  model?: 'text-embedding-3-small' | 'text-embedding-3-large';
  batchSize?: number;
  timeout?: number;
}

export class EmbeddingService {
  private openai: OpenAI;
  private batchSize: number;
  private model: string;

  constructor(config: EmbeddingServiceConfig) {
    this.openai = new OpenAI({
      apiKey: config.apiKey,
      timeout: config.timeout || 30000
    });
    this.batchSize = config.batchSize || 100;
    this.model = config.model || 'text-embedding-3-small';
  }

  async getEmbeddings(texts: string[]): Promise<number[][]> {
    if (texts.length === 0) return [];

    const embeddings: number[][] = [];
    const totalBatches = Math.ceil(texts.length / this.batchSize);

    for (let i = 0; i < texts.length; i += this.batchSize) {
      const batch = texts.slice(i, i + this.batchSize);
      const batchNum = i / this.batchSize + 1;

      console.log(`[Embedding] Processing batch ${batchNum}/${totalBatches} (${batch.length} texts)`);

      try {
        const response = await this.openai.embeddings.create({
          model: this.model,
          input: batch
        });

        const batchEmbeddings = response.data.map(d => d.embedding);
        embeddings.push(...batchEmbeddings);

      } catch (error) {
        console.error(`[Embedding] Batch ${batchNum} failed:`, error);
        throw error;
      }
    }

    console.log(`[Embedding] Completed: ${embeddings.length} embeddings, ${embeddings[0]?.length || 0} dimensions`);
    return embeddings;
  }

  getEmbedding(text: string): Promise<number[]> {
    return this.getEmbeddings([text]).then(embeds => embeddings[0]);
  }
}
```

### 2. ClusteringService.ts

```typescript
import { cosineDistance } from 'ml-distance';
import DBSCAN from 'density-clustering';

export interface ClusteringConfig {
  eps?: number;
  minSamples?: number;
  minClusterSize?: number;
}

export interface Cluster {
  representativeText: string;
  size: number;
  texts: string[];
  qualityScore?: number;
}

export class ClusteringService {
  private eps: number;
  private minSamples: number;
  private minClusterSize: number;

  constructor(config: ClusteringConfig = {}) {
    this.eps = config.eps ?? 0.25;
    this.minSamples = config.minSamples ?? 3;
    this.minClusterSize = config.minClusterSize ?? 3;
  }

  computeDistanceMatrix(embeddings: number[][]): number[][] {
    const n = embeddings.length;
    const matrix: number[][] = Array(n).fill(0).map(() => Array(n).fill(0));

    console.log(`[Clustering] Computing distance matrix for ${n} embeddings...`);

    for (let i = 0; i < n; i++) {
      for (let j = i + 1; j < n; j++) {
        const dist = cosineDistance(embeddings[i], embeddings[j]);
        matrix[i][j] = dist;
        matrix[j][i] = dist; // å¯¹ç§°çŸ©é˜µ
      }
    }

    return matrix;
  }

  cluster(embeddings: number[][], texts: string[]): Cluster[] {
    if (embeddings.length === 0) return [];

    console.log(`[Clustering] Starting DBSCAN clustering (eps=${this.eps}, minSamples=${this.minSamples})...`);

    // è®¡ç®—è·ç¦»çŸ©é˜µ
    const distanceMatrix = this.computeDistanceMatrix(embeddings);

    // DBSCAN èšç±»
    const dbscan = new DBSCAN();
    const labels = dbscan.train(distanceMatrix, this.eps, this.minSamples);

    // ç»Ÿè®¡èšç±»ç»“æœ
    const uniqueLabels = [...new Set(labels)];
    const nClusters = uniqueLabels.filter(l => l !== -1).length;
    const nNoise = labels.filter(l => l === -1).length;

    console.log(`[Clustering] Found ${nClusters} clusters, ${nNoise} noise points`);

    // æ„å»ºèšç±»ç»“æœ
    const clusterMap = new Map<number, number[]>();

    for (let i = 0; i < labels.length; i++) {
      const label = labels[i];
      if (label === -1) continue; // è·³è¿‡å™ªéŸ³ç‚¹

      if (!clusterMap.has(label)) {
        clusterMap.set(label, []);
      }
      clusterMap.get(label)!.push(i);
    }

    // è½¬æ¢ä¸ºè¾“å‡ºæ ¼å¼
    const clusters: Cluster[] = [];

    for (const [label, indices] of clusterMap.entries()) {
      if (indices.length < this.minClusterSize) continue;

      // æ‰¾åˆ°ä»£è¡¨æ€§æ–‡æœ¬ (è·ç¦»èšç±»ä¸­å¿ƒæœ€è¿‘çš„)
      const clusterEmbeddings = indices.map(i => embeddings[i]);
      const centroid = this.computeCentroid(clusterEmbeddings);

      let bestIdx = indices[0];
      let minDist = Infinity;

      for (const idx of indices) {
        const dist = cosineDistance([centroid], [embeddings[idx]]);
        if (dist < minDist) {
          minDist = dist;
          bestIdx = idx;
        }
      }

      clusters.push({
        representativeText: texts[bestIdx],
        size: indices.length,
        texts: indices.map(i => texts[i])
      });
    }

    // æŒ‰å¤§å°æ’åº
    clusters.sort((a, b) => b.size - a.size);

    console.log(`[Clustering] Returning ${clusters.length} clusters after filtering (min size=${this.minClusterSize})`);

    return clusters;
  }

  private computeCentroid(vectors: number[][]): number[] {
    const n = vectors.length;
    const dim = vectors[0].length;
    const centroid = new Array(dim).fill(0);

    for (const vector of vectors) {
      for (let i = 0; i < dim; i++) {
        centroid[i] += vector[i];
      }
    }

    for (let i = 0; i < dim; i++) {
      centroid[i] /= n;
    }

    return centroid;
  }

  // è‡ªé€‚åº”å‚æ•°è®¡ç®—
  static calculateAdaptiveParams(dataSize: number): { eps: number; minSamples: number } {
    let eps: number;
    let minSamples: number;

    if (dataSize < 20) {
      eps = 0.45;
      minSamples = 3;
    } else if (dataSize < 50) {
      eps = 0.38;
      minSamples = 3;
    } else if (dataSize < 100) {
      eps = 0.30;
      minSamples = 4;
    } else {
      eps = 0.25;
      minSamples = Math.max(5, Math.floor(dataSize / 50));
    }

    return { eps, minSamples };
  }
}
```

### 3. DataCleaner.ts

```typescript
export interface NoisePattern {
  regex: string;
  description: string;
}

export interface DataCleanerConfig {
  minLength?: number;
  customNoisePatterns?: NoisePattern[];
  customWhitelistKeywords?: string[];
}

export class DataCleaner {
  private static readonly DEFAULT_NOISE_PATTERNS: NoisePattern[] = [
    { regex: '^å“ˆ+$', description: 'çº¯å“ˆå“ˆå“ˆ' },
    { regex: '^å˜»+$', description: 'çº¯å˜»å˜»å˜»' },
    { regex: '^å‘µ+$', description: 'çº¯å‘µå‘µå‘µ' },
    { regex: '^[å¥½æ£’èµ]+$', description: 'çº¯å¥½æ£’èµ' },
    { regex: '^æ”¯æŒ+$', description: 'çº¯æ”¯æŒ' },
    { regex: '^åŠ æ²¹+$', description: 'çº¯åŠ æ²¹' },
    { regex: '^è¹²+$', description: 'çº¯è¹²' },
    { regex: '^@\S+', description: '@æŸäºº' },
    { regex: '^è½¬å‘å¾®åš', description: 'è½¬å‘å¾®åš' },
    { regex: '^å·²é˜…$', description: 'å·²é˜…' },
    { regex: '^mark$', description: 'mark (case-insensitive)' },
    { regex: '^æ”¶è—$', description: 'æ”¶è—' },
    { regex: '^[å•Šå“¦å—¯å””é¢]+$', description: 'çº¯è¯­æ°”è¯' },
    { regex: '^[\d\.]+$', description: 'çº¯æ•°å­—' },
    { regex: '^ğŸ‘â¤ï¸ğŸ’•ğŸ‰ğŸ˜€ğŸ˜ğŸ˜‚ğŸ¤£ğŸ˜ƒğŸ˜„ğŸ˜…ğŸ˜†ğŸ˜ŠğŸ˜‹ğŸ˜ğŸ’ªğŸ‘ğŸ™âœ¨ğŸŒŸâ­ï¸ğŸ”¥ğŸ’¯ğŸŠğŸğŸˆğŸŒˆâ˜€ï¸ğŸŒ™âš¡ï¸ğŸ’«\s*]+$', description: 'çº¯è¡¨æƒ…' },
  ];

  private static readonly DEFAULT_WHITELIST_KEYWORDS = [
    // é—®é¢˜è¡¨è¾¾
    'æ€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'ä¸ºå•¥', 'éš¾', 'å‘', 'éº»çƒ¦', 'å¯¼è‡´', 'é—®é¢˜', 'è§£å†³',
    // éœ€æ±‚è¡¨è¾¾
    'æ±‚', 'å¸Œæœ›', 'å»ºè®®', 'æ¨è', 'æƒ³è¦', 'éœ€è¦', 'èƒ½ä¸èƒ½', 'å¯ä»¥å—', 'æœ‰æ²¡æœ‰',
    // å­¦ä¹ å›°éš¾
    'ä¸æ‡‚', 'ä¸ä¼š', 'å­¦ä¸ä¼š', 'å¤ªéš¾', 'æä¸æ‡‚', 'çœ‹ä¸æ‡‚', 'ç†è§£ä¸äº†',
    // ä½“éªŒé—®é¢˜
    'åæ‚”', 'é¿é›·', 'è¸©å‘', 'è¢«å‘', 'ä¸å¥½ç”¨', 'å¤±æœ›', 'ç³Ÿç³•',
    // ä»·æ ¼æ•æ„Ÿ
    'è´µ', 'ä¾¿å®œ', 'å¹³æ›¿', 'æ›¿ä»£', 'çœé’±', 'åˆ’ç®—', 'æ€§ä»·æ¯”', 'å€¼å—', 'å€¼å¾—å—',
    // è´¨é‡æŠ•è¯‰
    'åæ§½', 'å·®è¯„', 'é€€æ¬¾', 'å”®å', 'å®¢æœ', 'è´¨é‡', 'åäº†', 'ä¸è¡Œ',
    // æŠ€æœ¯é—®é¢˜
    'bug', 'BUG', 'å¡', 'é—ªé€€', 'å´©æºƒ', 'æŠ¥é”™', 'å¼‚å¸¸', 'å¤±è´¥', 'æ— æ³•',
    // å¯¹æ¯”é€‰æ‹©
    'å“ªä¸ª', 'å“ªé‡Œ', 'é€‰æ‹©', 'åŒºåˆ«', 'å¯¹æ¯”', 'è¿˜æ˜¯',
    // æ•™ç¨‹æŒ‡å¯¼
    'æ•™ç¨‹', 'æ­¥éª¤', 'æ–¹æ³•', 'æ”»ç•¥', 'æŒ‡å—', 'æ•™å­¦',
  ];

  private noiseRegexes: RegExp[];
  private whitelistKeywords: string[];
  private minLength: number;

  constructor(config: DataCleanerConfig = {}) {
    this.minLength = config.minLength ?? 4;

    // ç¼–è¯‘å™ªéŸ³æ¨¡å¼
    this.noiseRegexes = DataCleaner.DEFAULT_NOISE_PATTERNS
      .concat(config.customNoisePatterns || [])
      .map(p => new RegExp(p.regex, 'u'));

    // è®¾ç½®ç™½åå•
    this.whitelistKeywords = DataCleaner.DEFAULT_WHITELIST_KEYWORDS
      .concat(config.customWhitelistKeywords || []);
  }

  isNoise(text: string): boolean {
    const trimmed = text.trim();

    // é•¿åº¦æ£€æŸ¥
    if (trimmed.length < this.minLength) return true;

    // å™ªéŸ³æ¨¡å¼åŒ¹é…
    for (const regex of this.noiseRegexes) {
      if (regex.test(trimmed)) return true;
    }

    return false;
  }

  hasWhitelistKeyword(text: string): boolean {
    return this.whitelistKeywords.some(keyword => text.includes(keyword));
  }

  calculateScore(text: string): number {
    let score = 1.0;
    const length = text.length;

    // ç™½åå•å…³é”®è¯åŠ æƒ
    if (this.hasWhitelistKeyword(text)) {
      score += 2.0;
    }

    // é•¿åº¦åŠ æƒ
    if (length >= 50 && length <= 200) {
      score += 1.0;
    } else if (length >= 20 && length < 50) {
      score += 0.5;
    } else if (length >= 10 && length < 20) {
      score += 0.2;
    } else if (length > 300) {
      score -= 0.5;
    }

    // é—®å·åŠ æƒ
    const questionMarks = (text.match(/\?/g) || []).length;
    if (questionMarks > 0) {
      const simpleQuestions = ['å•¥', 'ä»€ä¹ˆæ„æ€', 'çœŸçš„å—', 'æ˜¯å—', 'è¿™æ˜¯å•¥', 'è°å•Š'];
      const isSimpleQuestion = simpleQuestions.some(q => text.includes(q)) && length < 15;
      if (isSimpleQuestion) {
        score -= 1.0;
      } else {
        score += 0.3 * Math.min(questionMarks, 2);
      }
    }

    // åŒ…å«æ•°å­—åŠ æƒ
    if (/\d/.test(text)) {
      score += 0.3;
    }

    // æ„Ÿå¹å·è¿‡å¤šæ‰£åˆ†
    const exclamationMarks = (text.match(/!/g) || []).length;
    if (exclamationMarks > 2) {
      score -= 0.5;
    }

    return score;
  }

  clean(texts: string[]): { cleanedTexts: string[]; scores: number[] } {
    const cleaned: string[] = [];
    const scores: number[] = [];
    const seen = new Set<string>();

    for (const text of texts) {
      const trimmed = text.trim();

      if (!trimmed) continue;

      // å»é‡
      if (seen.has(trimmed)) continue;
      seen.add(trimmed);

      // è¿‡æ»¤å™ªéŸ³
      if (this.isNoise(trimmed)) continue;

      const score = this.calculateScore(trimmed);
      cleaned.push(trimmed);
      scores.push(score);
    }

    console.log(`[DataCleaner] Cleaned ${texts.length} -> ${cleaned.length} texts`);

    return { cleanedTexts: cleaned, scores };
  }
}
```

### 4. å®Œæ•´å·¥ä½œæµ

```typescript
// main.ts
import { EmbeddingService } from './EmbeddingService';
import { ClusteringService } from './ClusteringService';
import { DataCleaner } from './DataCleaner';

async function processTexts(
  texts: string[],
  options: {
    eps?: number;
    minSamples?: number;
    minLength?: number;
  } = {}
): Promise<any[]> {

  // 1. æ•°æ®æ¸…æ´—
  console.log('[Process] Step 1: Data cleaning...');
  const cleaner = new DataCleaner({ minLength: options.minLength ?? 4 });
  const { cleanedTexts, scores } = cleaner.clean(texts);

  if (cleanedTexts.length === 0) {
    console.warn('[Process] No valid texts after cleaning');
    return [];
  }

  // 2. è·å– embeddings
  console.log('[Process] Step 2: Getting embeddings...');
  const embeddingService = new EmbeddingService({
    apiKey: process.env.OPENAI_API_KEY!,
    model: 'text-embedding-3-small'
  });

  const embeddings = await embeddingService.getEmbeddings(cleanedTexts);

  // 3. è®¡ç®—è‡ªé€‚åº”å‚æ•°ï¼ˆå¦‚æœªæŒ‡å®šï¼‰
  const dataParams = ClusteringService.calculateAdaptiveParams(cleanedTexts.length);
  const eps = options.eps ?? dataParams.eps;
  const minSamples = options.minSamples ?? dataParams.minSamples;

  // 4. èšç±»
  console.log('[Process] Step 3: Clustering...');
  const clusteringService = new ClusteringService({ eps, minSamples });
  const clusters = clusteringService.cluster(embeddings, cleanedTexts);

  console.log(`[Process] Complete: ${clusters.length} clusters found`);
  return clusters;
}

// ä½¿ç”¨ç¤ºä¾‹
const texts = [
  "è¿™ä¸ªäº§å“çœŸçš„å¾ˆéš¾ç”¨ï¼Œå®¢æœä¹Ÿä¸ç†äºº",
  "æ±‚æ¨èä¸€æ¬¾æ€§ä»·æ¯”é«˜çš„å­¦ä¹ è½¯ä»¶",
  "æ€ä¹ˆä½¿ç”¨è¿™ä¸ªåŠŸèƒ½ï¼Ÿä¸å¤ªæ‡‚",
  "666",
  "å“ˆå“ˆå“ˆå¤ªå¥½ç¬‘äº†"
];

const clusters = await processTexts(texts, {
  minLength: 4
});

console.log(clusters);
```

---

## æ€»ç»“

### å…³é”®å†³ç­–çŸ©é˜µ

| å†³ç­–å› ç´  | Python (æ™ºè°±AI) | TypeScript (OpenAI) | æ¨è |
|----------|----------------|-------------------|------|
| **æˆæœ¬** | è¾ƒé«˜ | æä½ | âœ… TypeScript |
| **é€Ÿåº¦** | æ…¢ | å¿« | âœ… TypeScript |
| **è´¨é‡** | è‰¯å¥½ | ä¼˜ç§€ | âœ… TypeScript |
| **ç»´æŠ¤æ€§** | åŒè¯­è¨€å¤æ‚ | å•è¯­è¨€ç®€å• | âœ… TypeScript |
| **ç¦»çº¿èƒ½åŠ›** | æ”¯æŒ | éœ€é¢å¤–æ–¹æ¡ˆ | Python |
| **æˆç†Ÿåº¦** | æˆç†Ÿ | æˆç†Ÿ | ç›¸å½“ |

### æœ€ç»ˆæ¨è

**å¼ºçƒˆæ¨è:** ä½¿ç”¨ TypeScript + OpenAI + density-clustering å®Œå…¨æ›¿ä»£ Python æ–¹æ¡ˆ

**å®æ–½æ—¶é—´:** 5-9 ä¸ªå·¥ä½œæ—¥

**é¢„æœŸæ”¶ç›Š:**
- æˆæœ¬é™ä½ 90%
- æ€§èƒ½æå‡ 3-5 å€
- ç»´æŠ¤å¤æ‚åº¦é™ä½ 50%
- ç»Ÿä¸€æŠ€æœ¯æ ˆ

**é£é™©:** ä½ - å¯é€šè¿‡å¹¶è¡Œè¿è¡Œå’Œç°åº¦å‘å¸ƒå®Œå…¨æ¶ˆé™¤é£é™©

---

*æŠ¥å‘Šç‰ˆæœ¬: 1.0*
*ç”Ÿæˆæ—¥æœŸ: 2025å¹´*
*ä½œè€…: DeepPoint AI æŠ€æœ¯å›¢é˜Ÿ*
