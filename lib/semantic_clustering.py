#!/usr/bin/env python3
"""
è¯­ä¹‰èšç±»æœåŠ¡ - åŸºäºæ™ºè°±AI Embedding + DBSCAN
æ›¿ä»£åŸæœ‰çš„ Jaccard èšç±»ç®—æ³•ï¼Œæä¾›æ›´å¥½çš„è¯­ä¹‰ç›¸ä¼¼åº¦èšç±»
"""

import json
import sys
import os
import re
import time
import logging
from typing import List, Dict, Optional, Tuple
from pathlib import Path

import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import cosine_distances
from sklearn.metrics import silhouette_score, davies_bouldin_score

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
env_path = Path(__file__).parent.parent
load_dotenv(env_path / '.env.local')
load_dotenv(env_path / '.env')

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


# ==================== æ•°æ®æ¸…æ´—æ¨¡å— ====================

class DataCleaner:
    """é«˜ä¿¡å™ªæ¯”æ•°æ®æ¸…æ´—å™¨"""

    # æ— æ•ˆç¤¾äº¤ç”¨è¯­ï¼ˆä¼šè¢«ç›´æ¥è¿‡æ»¤ï¼‰
    NOISE_PATTERNS = [
        r'^å“ˆ+$',           # çº¯å“ˆå“ˆå“ˆ
        r'^å˜»+$',           # çº¯å˜»å˜»å˜»
        r'^å‘µ+$',           # çº¯å‘µå‘µå‘µ
        r'^[å¥½æ£’èµ]+$',     # çº¯å¥½æ£’èµ
        r'^æ”¯æŒ+$',
        r'^åŠ æ²¹+$',
        r'^è¹²+$',
        r'^@\S+',           # @æŸäºº
        r'^è½¬å‘å¾®åš',
        r'^å·²é˜…$',
        r'^mark$',
        r'^[Mm]ark$',
        r'^æ”¶è—$',
        r'^[å•Šå“¦å—¯å””é¢]+$',  # çº¯è¯­æ°”è¯
        r'^[\d\.]+$',       # çº¯æ•°å­—
        r'^ğŸ‘â¤ï¸ğŸ’•ğŸ‰ğŸ˜€ğŸ˜ğŸ˜‚ğŸ¤£ğŸ˜ƒğŸ˜„ğŸ˜…ğŸ˜†ğŸ˜ŠğŸ˜‹ğŸ˜ğŸ’ªğŸ‘ğŸ™âœ¨ğŸŒŸâ­ï¸ğŸ”¥ğŸ’¯ğŸŠğŸğŸˆğŸŒˆâ˜€ï¸ğŸŒ™âš¡ï¸ğŸ’«\s*]+$',  # çº¯è¡¨æƒ…
    ]

    # æ— æ•ˆçŸ­è¯­åˆ—è¡¨ï¼ˆé€šç”¨ç¤¾äº¤è¡¨è¾¾ï¼‰
    NOISE_PHRASES = [
        # çº¯èµç¾
        'å¥½å¬', 'å¥½çœ‹', 'çœŸå¥½', 'ä¸é”™', 'å¯ä»¥', 'å‰å®³', 'ç‰›', 'ç»äº†', 'å¤ªæ£’äº†', 'å¤ªå¼ºäº†',
        '666', 'ç‰›é€¼', 'ç‰›æ‰¹', 'çœŸæ£’', 'çœŸå¥½', 'çœŸä¸é”™',

        # çº¯æƒ…ç»ª
        'å“ˆå“ˆå“ˆ', 'å“ˆå“ˆå“ˆå“ˆ', 'ç¬‘æ­»', 'ç¬‘äº†', 'å¤ªå¥½ç¬‘', 'ç»ç»å­', 'å“­äº†', 'çˆ±äº†',

        # è¿½æ›´ç±»
        'è¹²', 'è¹²ä¸€ä¸ª', 'ç­‰æ›´æ–°', 'å‚¬æ›´', 'ä»€ä¹ˆæ—¶å€™æ›´æ–°',

        # å ä½ç±»
        'ç¬¬ä¸€', 'æ²™å‘', 'å‰æ’', 'æ¥äº†', 'æ‰“å¡', 'ç­¾åˆ°', 'è·¯è¿‡',

        # å£å·ç±»
        'æ”¯æŒ', 'åŠ æ²¹', 'å†²', 'å†²å†²å†²', 'å¥¥åˆ©ç»™', 'yyds', 'æ°¸è¿œçš„ç¥',

        # æ— å®è´¨å†…å®¹çš„ç–‘é—®
        'å•¥', 'å•¥æ„æ€', 'ä»€ä¹ˆæ„æ€', 'å•¥ç©æ„', 'è¿™æ˜¯å•¥', 'çœŸçš„å—', 'çœŸå‡',
        'æ˜¯å—', 'å—', 'å‘¢', 'å§', 'å‘€', 'å•Š',

        # èº«ä»½è¯¢é—®/æ— å…³è¯„è®º
        'æ˜¯è°', 'è°å•Š', 'ä¸è®¤è¯†', 'è¿™è°', 'åšä¸»æ˜¯è°',

        # å¹¿å‘Šè¥é”€ç›¸å…³
        'ç§ä¿¡', 'ç§ä¿¡æˆ‘', 'åŠ V', 'åŠ å¾®ä¿¡', 'åŠ å¥½å‹', 'ç‚¹å‡»é“¾æ¥', 'æ‰«ç ',
    ]

    # ç™½åå•å…³é”®è¯ï¼ˆä¼˜å…ˆä¿ç•™ï¼‰- é€šç”¨ç—›ç‚¹ç›¸å…³è¯æ±‡
    WHITELIST_KEYWORDS = [
        # é—®é¢˜è¡¨è¾¾
        'æ€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'ä¸ºå•¥', 'éš¾', 'å‘', 'éº»çƒ¦', 'å¯¼è‡´', 'é—®é¢˜', 'è§£å†³',

        # éœ€æ±‚è¡¨è¾¾
        'æ±‚', 'å¸Œæœ›', 'å»ºè®®', 'æ¨è', 'æƒ³è¦', 'éœ€è¦', 'èƒ½ä¸èƒ½', 'å¯ä»¥å—', 'æœ‰æ²¡æœ‰',

        # å­¦ä¹ å›°éš¾
        'ä¸æ‡‚', 'ä¸ä¼š', 'å­¦ä¸ä¼š', 'å¤ªéš¾', 'æä¸æ‡‚', 'çœ‹ä¸æ‡‚', 'ç†è§£ä¸äº†',

        # ä½“éªŒé—®é¢˜
        'åæ‚”', 'é¿é›·', 'è¸©å‘', 'è¢«å‘', 'ä¸å¥½ç”¨', 'å¤±æœ›', 'ç³Ÿç³•',

        # ä»·æ ¼æ•æ„Ÿ
        'è´µ', 'ä¾¿å®œ', 'å¹³æ›¿', 'æ›¿ä»£', 'çœé’±', 'åˆ’ç®—', 'æ€§ä»·æ¯”', 'å€¼å—', 'å€¼å¾—å—',

        # è´¨é‡æŠ•è¯‰
        'åæ§½', 'å·®è¯„', 'é€€æ¬¾', 'å”®å', 'å®¢æœ', 'è´¨é‡', 'åäº†', 'ä¸è¡Œ',

        # æŠ€æœ¯é—®é¢˜
        'bug', 'BUG', 'å¡', 'é—ªé€€', 'å´©æºƒ', 'æŠ¥é”™', 'å¼‚å¸¸', 'å¤±è´¥', 'æ— æ³•',

        # å¯¹æ¯”é€‰æ‹©
        'å“ªä¸ª', 'å“ªé‡Œ', 'é€‰æ‹©', 'åŒºåˆ«', 'å¯¹æ¯”', 'è¿˜æ˜¯',

        # æ•™ç¨‹æŒ‡å¯¼
        'æ•™ç¨‹', 'æ­¥éª¤', 'æ–¹æ³•', 'æ”»ç•¥', 'æŒ‡å—', 'æ•™å­¦',
    ]

    def __init__(self, min_length: int = 4):
        self.min_length = min_length
        self.noise_regexes = [re.compile(p) for p in self.NOISE_PATTERNS]

    def is_noise(self, text: str) -> bool:
        """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸ºå™ªéŸ³"""
        text = text.strip()

        # é•¿åº¦è¿‡çŸ­
        if len(text) < self.min_length:
            return True

        # åŒ¹é…å™ªéŸ³æ­£åˆ™
        for regex in self.noise_regexes:
            if regex.match(text):
                return True

        # åŒ¹é…å™ªéŸ³çŸ­è¯­
        text_lower = text.lower()
        for phrase in self.NOISE_PHRASES:
            if text_lower == phrase.lower():
                return True

        return False

    def has_whitelist_keyword(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«ç™½åå•å…³é”®è¯"""
        for keyword in self.WHITELIST_KEYWORDS:
            if keyword in text:
                return True
        return False

    def calculate_score(self, text: str) -> float:
        """
        è®¡ç®—æ–‡æœ¬è´¨é‡åˆ†æ•°ï¼ˆç”¨äºæ’åºå’Œç­›é€‰ä»£è¡¨æ€§æ–‡æœ¬ï¼‰
        åˆ†æ•°è¶Šé«˜ï¼Œæ–‡æœ¬è¶Šæœ‰ä»·å€¼
        """
        score = 1.0
        length = len(text)

        # ç™½åå•å…³é”®è¯åŠ æƒï¼ˆç—›ç‚¹ç›¸å…³ï¼‰
        if self.has_whitelist_keyword(text):
            score += 2.0

        # é•¿åº¦åŠ æƒï¼ˆ50-200å­—ç¬¦æœ€ä½³ï¼Œæœ‰å®è´¨å†…å®¹ï¼‰
        if 50 <= length <= 200:
            score += 1.0
        elif 20 <= length < 50:
            score += 0.5
        elif 10 <= length < 20:
            score += 0.2
        elif length > 300:
            score -= 0.5  # è¿‡é•¿çš„æ–‡æœ¬å¯èƒ½æ˜¯å¤åˆ¶ç²˜è´´

        # åŒ…å«é—®å·åŠ æƒï¼ˆå¯èƒ½æ˜¯çœŸå®é—®é¢˜ï¼‰
        question_marks = text.count('?') + text.count('ï¼Ÿ')
        if question_marks > 0:
            # ä½†å¦‚æœæ˜¯çº¯ç–‘é—®è¯+é—®å·ï¼ˆæ— å®è´¨å†…å®¹ï¼‰ï¼Œåˆ™æ‰£åˆ†
            simple_questions = ['å•¥', 'ä»€ä¹ˆæ„æ€', 'çœŸçš„å—', 'æ˜¯å—', 'è¿™æ˜¯å•¥', 'è°å•Š']
            is_simple_question = any(q in text for q in simple_questions) and length < 15
            if is_simple_question:
                score -= 1.0
            else:
                score += 0.3 * min(question_marks, 2)  # æœ€å¤šåŠ 0.6åˆ†

        # åŒ…å«æ•°å­—åŠ æƒï¼ˆå¯èƒ½åŒ…å«å…·ä½“æ•°æ®/ä»·æ ¼ï¼‰
        if re.search(r'\d+', text):
            score += 0.3

        # åŒ…å«æ„Ÿå¹å·è¿‡å¤šæ‰£åˆ†ï¼ˆå¯èƒ½æ˜¯æƒ…ç»ªåŒ–è¡¨è¾¾ï¼‰
        exclamation_marks = text.count('!') + text.count('ï¼')
        if exclamation_marks > 2:
            score -= 0.5

        return score

    def clean(self, texts: List[str]) -> Tuple[List[str], List[float]]:
        """
        æ¸…æ´—æ–‡æœ¬åˆ—è¡¨
        è¿”å›: (æ¸…æ´—åçš„æ–‡æœ¬åˆ—è¡¨, å¯¹åº”çš„è´¨é‡åˆ†æ•°)
        """
        cleaned = []
        scores = []
        seen = set()  # å»é‡

        for text in texts:
            text = text.strip()

            # è·³è¿‡ç©ºæ–‡æœ¬
            if not text:
                continue

            # å»é‡
            if text in seen:
                continue
            seen.add(text)

            # è·³è¿‡å™ªéŸ³
            if self.is_noise(text):
                continue

            score = self.calculate_score(text)
            cleaned.append(text)
            scores.append(score)

        logger.info(f"æ•°æ®æ¸…æ´—å®Œæˆ: {len(texts)} -> {len(cleaned)} æ¡ (è¿‡æ»¤äº† {len(texts) - len(cleaned)} æ¡å™ªéŸ³)")
        return cleaned, scores


# ==================== Embedding æ¨¡å— ====================

class ZhipuEmbedding:
    """æ™ºè°±AI Embedding æœåŠ¡"""

    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv('GLM_API_KEY')
        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ° GLM_API_KEYï¼Œè¯·åœ¨ .env.local æˆ–ç¯å¢ƒå˜é‡ä¸­é…ç½®")

        self.base_url = "https://open.bigmodel.cn/api/paas/v4/embeddings"
        self.model = os.getenv('GLM_EMBEDDING_MODEL', 'embedding-3')
        self.batch_size = 25  # æ¯æ‰¹æœ€å¤§æ•°é‡
        self.rate_limit_delay = 0.5  # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰

    def _get_embedding_batch(self, texts: List[str]) -> List[List[float]]:
        """è·å–ä¸€æ‰¹æ–‡æœ¬çš„ embedding"""
        import urllib.request
        import urllib.error

        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }

        # æ™ºè°± API éœ€è¦é€ä¸ªè¯·æ±‚
        embeddings = []
        for text in texts:
            data = json.dumps({
                'model': self.model,
                'input': text
            }).encode('utf-8')

            req = urllib.request.Request(self.base_url, data=data, headers=headers)

            try:
                with urllib.request.urlopen(req, timeout=30) as response:
                    result = json.loads(response.read().decode('utf-8'))
                    embedding = result['data'][0]['embedding']
                    embeddings.append(embedding)
            except urllib.error.HTTPError as e:
                error_body = e.read().decode('utf-8')
                logger.error(f"API é”™è¯¯: {e.code} - {error_body}")
                raise
            except Exception as e:
                logger.error(f"è¯·æ±‚å¤±è´¥: {e}")
                raise

            # æ·»åŠ å»¶è¿Ÿé¿å…é™æµ
            time.sleep(self.rate_limit_delay)

        return embeddings

    def get_embeddings(self, texts: List[str]) -> np.ndarray:
        """
        è·å–æ–‡æœ¬åˆ—è¡¨çš„ embedding å‘é‡
        è‡ªåŠ¨å¤„ç†æ‰¹é‡è¯·æ±‚å’Œé™æµ
        """
        if not texts:
            return np.array([])

        all_embeddings = []
        total_batches = (len(texts) + self.batch_size - 1) // self.batch_size

        for i in range(0, len(texts), self.batch_size):
            batch = texts[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1
            logger.info(f"æ­£åœ¨è·å– Embedding: æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} æ¡)")

            embeddings = self._get_embedding_batch(batch)
            all_embeddings.extend(embeddings)

        return np.array(all_embeddings)


# ==================== èšç±»æ¨¡å— ====================

def optimize_clustering_params(
    embeddings: np.ndarray,
    eps_range: List[float] = [0.2, 0.25, 0.3],
    min_samples_range: Optional[List[int]] = None
) -> Tuple[float, int]:
    """
    è‡ªåŠ¨ä¼˜åŒ–DBSCANèšç±»å‚æ•°

    å‚æ•°:
        embeddings: å‘é‡çŸ©é˜µ
        eps_range: epså€™é€‰å€¼åˆ—è¡¨
        min_samples_range: min_sampleså€™é€‰å€¼åˆ—è¡¨ï¼ˆä¸ºNoneæ—¶è‡ªåŠ¨ç”Ÿæˆï¼‰

    è¿”å›:
        (æœ€ä¼˜eps, æœ€ä¼˜min_samples)
    """
    if len(embeddings) < 10:
        logger.warning("æ•°æ®é‡å¤ªå°‘ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
        return 0.25, 3

    # è‡ªåŠ¨ç”Ÿæˆmin_sampleså€™é€‰å€¼
    if min_samples_range is None:
        base_min_samples = max(3, len(embeddings) // 50)
        min_samples_range = [
            max(2, base_min_samples - 1),
            base_min_samples,
            base_min_samples + 1
        ]

    logger.info(f"å¼€å§‹å‚æ•°ä¼˜åŒ–ï¼šå°è¯• {len(eps_range)} Ã— {len(min_samples_range)} = {len(eps_range) * len(min_samples_range)} ç»„å‚æ•°")

    # é¢„è®¡ç®—è·ç¦»çŸ©é˜µï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
    distance_matrix = cosine_distances(embeddings)

    best_score = -1
    best_params = (0.25, 3)
    best_n_clusters = 0

    for eps in eps_range:
        for min_samples in min_samples_range:
            try:
                # æ‰§è¡Œèšç±»
                dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed')
                labels = dbscan.fit_predict(distance_matrix)

                # ç»Ÿè®¡èšç±»æ•°é‡
                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                n_noise = list(labels).count(-1)

                # è‡³å°‘éœ€è¦2ä¸ªèšç±»æ‰èƒ½è®¡ç®—silhouette
                if n_clusters < 2:
                    continue

                # è¿‡æ»¤å™ªéŸ³ç‚¹
                non_noise_mask = labels != -1
                if np.sum(non_noise_mask) <= n_clusters:
                    continue

                # è®¡ç®—silhouette score
                score = silhouette_score(
                    embeddings[non_noise_mask],
                    labels[non_noise_mask],
                    metric='cosine'
                )

                logger.info(f"  eps={eps}, min_samples={min_samples}: {n_clusters}ç°‡, {n_noise}å™ªéŸ³, score={score:.4f}")

                # æ›´æ–°æœ€ä¼˜å‚æ•°ï¼ˆä¼˜å…ˆè€ƒè™‘scoreï¼Œå…¶æ¬¡è€ƒè™‘èšç±»æ•°é‡ï¼‰
                if score > best_score or (score == best_score and n_clusters > best_n_clusters):
                    best_score = score
                    best_params = (eps, min_samples)
                    best_n_clusters = n_clusters

            except Exception as e:
                logger.debug(f"å‚æ•° eps={eps}, min_samples={min_samples} å¤±è´¥: {e}")
                continue

    if best_score > -1:
        logger.info(f"âœ“ æ‰¾åˆ°æœ€ä¼˜å‚æ•°: eps={best_params[0]}, min_samples={best_params[1]}, score={best_score:.4f}")
        return best_params
    else:
        logger.warning("å‚æ•°ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
        return 0.25, max(3, len(embeddings) // 50)


class SemanticClusterer:
    """åŸºäº DBSCAN çš„è¯­ä¹‰èšç±»å™¨"""

    def __init__(self, eps: float = 0.25, min_samples: int = 3):
        """
        å‚æ•°:
            eps: DBSCAN çš„é‚»åŸŸåŠå¾„ï¼ˆåŸºäºä½™å¼¦è·ç¦»ï¼Œ0.2-0.3 è¾ƒå¥½ï¼Œé™ä½ä»¥æé«˜èšç±»ä¸¥æ ¼åº¦ï¼‰
            min_samples: å½¢æˆèšç±»çš„æœ€å°æ ·æœ¬æ•°ï¼ˆå»ºè®®æ ¹æ®æ•°æ®é‡åŠ¨æ€è°ƒæ•´ï¼‰
        """
        self.eps = eps
        self.min_samples = min_samples

    def cluster(self, embeddings: np.ndarray, texts: List[str], scores: Optional[List[float]] = None) -> List[Dict]:
        """
        æ‰§è¡Œ DBSCAN èšç±»

        è¿”å›æ ¼å¼:
        [
            {
                "representative_text": "è·ç¦»èšç±»ä¸­å¿ƒæœ€è¿‘çš„æ–‡æœ¬",
                "size": èšç±»å¤§å°,
                "texts": ["æ–‡æœ¬1", "æ–‡æœ¬2", ...]
            },
            ...
        ]
        """
        if len(embeddings) == 0:
            return []

        if scores is None:
            scores = [1.0] * len(texts)

        # è®¡ç®—ä½™å¼¦è·ç¦»çŸ©é˜µ
        logger.info("æ­£åœ¨è®¡ç®—ä½™å¼¦è·ç¦»çŸ©é˜µ...")
        distance_matrix = cosine_distances(embeddings)

        # DBSCAN èšç±»
        logger.info(f"æ­£åœ¨æ‰§è¡Œ DBSCAN èšç±» (eps={self.eps}, min_samples={self.min_samples})...")
        dbscan = DBSCAN(eps=self.eps, min_samples=self.min_samples, metric='precomputed')
        labels = dbscan.fit_predict(distance_matrix)

        # ç»Ÿè®¡èšç±»ç»“æœ
        unique_labels = set(labels)
        n_clusters = len([l for l in unique_labels if l != -1])
        n_noise = list(labels).count(-1)
        logger.info(f"èšç±»å®Œæˆ: {n_clusters} ä¸ªèšç±», {n_noise} ä¸ªå™ªéŸ³ç‚¹")

        # è®¡ç®—èšç±»è´¨é‡æŒ‡æ ‡ï¼ˆä»…å½“æœ‰å¤šä¸ªèšç±»æ—¶ï¼‰
        if n_clusters > 1:
            # è¿‡æ»¤æ‰å™ªéŸ³ç‚¹ç”¨äºè¯„ä¼°
            non_noise_mask = labels != -1
            if np.sum(non_noise_mask) > n_clusters:
                try:
                    # Silhouetteåˆ†æ•°ï¼š-1åˆ°1ï¼Œè¶Šæ¥è¿‘1è¶Šå¥½
                    silhouette = silhouette_score(
                        embeddings[non_noise_mask],
                        labels[non_noise_mask],
                        metric='cosine'
                    )
                    logger.info(f"Silhouette Score: {silhouette:.4f} (èŒƒå›´: -1åˆ°1, è¶Šé«˜è¶Šå¥½)")

                    # Davies-BouldinæŒ‡æ•°ï¼šè¶Šå°è¶Šå¥½ï¼Œ0ä¸ºæœ€ä¼˜
                    db_index = davies_bouldin_score(
                        embeddings[non_noise_mask],
                        labels[non_noise_mask]
                    )
                    logger.info(f"Davies-Bouldin Index: {db_index:.4f} (è¶Šå°è¶Šå¥½)")

                    # è´¨é‡è¯„ä¼°æç¤º
                    if silhouette > 0.5:
                        logger.info("âœ“ èšç±»è´¨é‡ï¼šä¼˜ç§€ï¼ˆç°‡é—´åˆ†ç¦»åº¦é«˜ï¼‰")
                    elif silhouette > 0.3:
                        logger.info("âœ“ èšç±»è´¨é‡ï¼šè‰¯å¥½ï¼ˆç°‡è¾ƒä¸ºæ˜ç¡®ï¼‰")
                    elif silhouette > 0.1:
                        logger.info("âš  èšç±»è´¨é‡ï¼šä¸€èˆ¬ï¼ˆç°‡è¾¹ç•Œæ¨¡ç³Šï¼‰")
                    else:
                        logger.warning("âš  èšç±»è´¨é‡ï¼šè¾ƒå·®ï¼ˆå¯èƒ½éœ€è¦è°ƒæ•´å‚æ•°ï¼‰")

                except Exception as e:
                    logger.warning(f"æ— æ³•è®¡ç®—èšç±»è´¨é‡æŒ‡æ ‡: {e}")
        else:
            logger.warning("èšç±»æ•°é‡è¿‡å°‘ï¼Œæ— æ³•è®¡ç®—è´¨é‡æŒ‡æ ‡")

        # æ„å»ºèšç±»ç»“æœ
        clusters = {}
        for idx, label in enumerate(labels):
            if label == -1:  # å™ªéŸ³ç‚¹å•ç‹¬å¤„ç†
                continue
            if label not in clusters:
                clusters[label] = {
                    'indices': [],
                    'texts': [],
                    'scores': []
                }
            clusters[label]['indices'].append(idx)
            clusters[label]['texts'].append(texts[idx])
            clusters[label]['scores'].append(scores[idx])

        # ä¸ºæ¯ä¸ªèšç±»æ‰¾å‡ºä»£è¡¨æ€§æ–‡æœ¬
        results = []
        for label, cluster_data in clusters.items():
            indices = cluster_data['indices']
            cluster_texts = cluster_data['texts']
            cluster_scores = cluster_data['scores']

            # è®¡ç®—èšç±»ä¸­å¿ƒ
            cluster_embeddings = embeddings[indices]
            centroid = np.mean(cluster_embeddings, axis=0)

            # æ‰¾åˆ°è·ç¦»ä¸­å¿ƒæœ€è¿‘çš„æ–‡æœ¬
            distances_to_center = cosine_distances([centroid], cluster_embeddings)[0]

            # ç»¼åˆè·ç¦»å’Œè´¨é‡åˆ†æ•°é€‰æ‹©ä»£è¡¨æ–‡æœ¬
            combined_scores = []
            for i, (dist, score) in enumerate(zip(distances_to_center, cluster_scores)):
                # è·ç¦»è¶Šå°è¶Šå¥½ï¼ˆå–åï¼‰ï¼Œåˆ†æ•°è¶Šé«˜è¶Šå¥½
                combined = -dist + score * 0.3
                combined_scores.append(combined)

            best_idx = np.argmax(combined_scores)
            representative_text = cluster_texts[best_idx]

            # å»é‡åçš„æ–‡æœ¬åˆ—è¡¨
            unique_texts = list(dict.fromkeys(cluster_texts))

            results.append({
                'representative_text': representative_text,
                'size': len(unique_texts),
                'texts': unique_texts
            })

        # æŒ‰èšç±»å¤§å°æ’åº
        results.sort(key=lambda x: x['size'], reverse=True)

        # åå¤„ç†ï¼šè¿‡æ»¤æ‰è¿‡å°çš„èšç±»ï¼ˆè‡³å°‘éœ€è¦3æ¡æ•°æ®æ‰æœ‰ç»Ÿè®¡æ„ä¹‰ï¼‰
        min_cluster_size = 3
        filtered_results = [r for r in results if r['size'] >= min_cluster_size]

        if len(filtered_results) < len(results):
            removed_count = len(results) - len(filtered_results)
            logger.info(f"è¿‡æ»¤æ‰ {removed_count} ä¸ªè¿‡å°èšç±»ï¼ˆsize < {min_cluster_size}ï¼‰ï¼Œä¿ç•™ {len(filtered_results)} ä¸ªæœ‰æ„ä¹‰çš„èšç±»")

        # è®°å½•è¢«è¿‡æ»¤çš„é«˜è´¨é‡å™ªéŸ³ç‚¹æ•°é‡ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        noise_indices = [i for i, label in enumerate(labels) if label == -1]
        high_quality_noise = sum(1 for idx in noise_indices if scores[idx] >= 2.0)
        if high_quality_noise > 0:
            logger.info(f"è¿‡æ»¤æ‰ {high_quality_noise} ä¸ªé«˜è´¨é‡ä½†æœªèšç±»çš„æ–‡æœ¬ï¼ˆå¯è€ƒè™‘æ”¾å®½å‚æ•°ä»¥è·å¾—æ›´å¤šèšç±»ï¼‰")

        return filtered_results


# ==================== ä¸»æµç¨‹ ====================

def process_texts(
    texts: List[str],
    eps: Optional[float] = None,
    min_samples: Optional[int] = None,
    min_length: int = 4,
    auto_optimize: bool = False
) -> List[Dict]:
    """
    å®Œæ•´çš„æ–‡æœ¬å¤„ç†æµç¨‹ï¼šæ¸…æ´— -> Embedding -> èšç±»

    å‚æ•°:
        texts: æ–‡æœ¬åˆ—è¡¨
        eps: DBSCANé‚»åŸŸåŠå¾„ï¼ˆNoneæ—¶è‡ªåŠ¨è®¡ç®—ï¼‰
        min_samples: æœ€å°æ ·æœ¬æ•°ï¼ˆNoneæ—¶è‡ªåŠ¨è®¡ç®—ï¼‰
        min_length: æœ€å°æ–‡æœ¬é•¿åº¦
        auto_optimize: æ˜¯å¦è‡ªåŠ¨ä¼˜åŒ–èšç±»å‚æ•°
    """
    if not texts:
        return []

    # 1. æ•°æ®æ¸…æ´—
    logger.info("å¼€å§‹æ•°æ®æ¸…æ´—...")
    cleaner = DataCleaner(min_length=min_length)
    cleaned_texts, scores = cleaner.clean(texts)

    if not cleaned_texts:
        logger.warning("æ¸…æ´—åæ²¡æœ‰æœ‰æ•ˆæ–‡æœ¬")
        return []

    # 2. è·å– Embedding
    logger.info("å¼€å§‹è·å– Embedding...")
    embedder = ZhipuEmbedding()
    embeddings = embedder.get_embeddings(cleaned_texts)

    # 3. å‚æ•°ä¼˜åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if auto_optimize:
        logger.info("å¯ç”¨å‚æ•°è‡ªåŠ¨ä¼˜åŒ–...")
        eps, min_samples = optimize_clustering_params(embeddings)
    else:
        # åŠ¨æ€è®¡ç®— epsï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
        if eps is None:
            # æ ¹æ®æ•°æ®é‡è‡ªé€‚åº”è°ƒæ•´ eps
            data_size = len(cleaned_texts)
            if data_size < 20:
                eps = 0.45  # æå°æ•°æ®é›†ï¼šéå¸¸å®½æ¾
                logger.info(f"æå°æ•°æ®é›†({data_size}æ¡)ï¼Œä½¿ç”¨è¾ƒå¤§eps={eps}ä»¥ç¡®ä¿èƒ½å½¢æˆèšç±»")
            elif data_size < 50:
                eps = 0.38  # å°æ•°æ®é›†ï¼šè¾ƒå®½æ¾
                logger.info(f"å°æ•°æ®é›†({data_size}æ¡)ï¼Œä½¿ç”¨è¾ƒå¤§eps={eps}ä»¥é¿å…è¿‡åº¦åˆ†æ•£")
            elif data_size < 100:
                eps = 0.30  # ä¸­ç­‰æ•°æ®é›†
                logger.info(f"ä¸­ç­‰æ•°æ®é›†({data_size}æ¡)ï¼Œä½¿ç”¨ä¸­ç­‰eps={eps}")
            else:
                eps = 0.25  # å¤§æ•°æ®é›†ï¼šæ›´ä¸¥æ ¼
                logger.info(f"å¤§æ•°æ®é›†({data_size}æ¡)ï¼Œä½¿ç”¨è¾ƒå°eps={eps}ä»¥æé«˜èšç±»ä¸¥æ ¼åº¦")

        # åŠ¨æ€è®¡ç®— min_samplesï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
        if min_samples is None:
            # æ ¹æ®æ•°æ®é‡è‡ªé€‚åº”ï¼šè‡³å°‘3ä¸ªæ ·æœ¬ä»¥ä¿è¯èšç±»æœ‰æ„ä¹‰
            data_size = len(cleaned_texts)
            if data_size < 15:
                min_samples = 3  # æå°æ•°æ®é›†ï¼šä»ç„¶ä¿æŒ3ï¼Œé¿å…2æ¡å°±æˆç°‡
            elif data_size < 50:
                min_samples = 3  # å°æ•°æ®é›†
            elif data_size < 100:
                min_samples = 4  # ä¸­ç­‰æ•°æ®é›†æé«˜è¦æ±‚
            else:
                min_samples = max(5, data_size // 50)  # å¤§æ•°æ®é›†ï¼šè‡³å°‘5
            logger.info(f"åŠ¨æ€è®¡ç®— min_samples: {min_samples} (åŸºäº {data_size} æ¡æ¸…æ´—åçš„æ–‡æœ¬)")

    # 4. DBSCAN èšç±»
    logger.info("å¼€å§‹è¯­ä¹‰èšç±»...")
    clusterer = SemanticClusterer(eps=eps, min_samples=min_samples)
    clusters = clusterer.cluster(embeddings, cleaned_texts, scores)

    logger.info(f"å¤„ç†å®Œæˆï¼Œå…± {len(clusters)} ä¸ªèšç±»")
    return clusters


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse

    parser = argparse.ArgumentParser(description='è¯­ä¹‰èšç±»æœåŠ¡')
    parser.add_argument('--input', '-i', type=str, help='è¾“å…¥ JSON æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', '-o', type=str, help='è¾“å‡º JSON æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--eps', type=float, default=None, help='DBSCAN eps å‚æ•°ï¼ˆé‚»åŸŸåŠå¾„ï¼Œä¸ºNoneæ—¶æ ¹æ®æ•°æ®é‡è‡ªåŠ¨è°ƒæ•´ï¼‰')
    parser.add_argument('--min-samples', type=int, default=None, help='DBSCAN min_samples å‚æ•°ï¼ˆä¸ºNoneæ—¶è‡ªåŠ¨è®¡ç®—ï¼‰')
    parser.add_argument('--min-length', type=int, default=4, help='æœ€å°æ–‡æœ¬é•¿åº¦')
    parser.add_argument('--auto-optimize', action='store_true', help='è‡ªåŠ¨ä¼˜åŒ–èšç±»å‚æ•°')
    parser.add_argument('--stdin', action='store_true', help='ä»æ ‡å‡†è¾“å…¥è¯»å– JSON')

    args = parser.parse_args()

    # è¯»å–è¾“å…¥
    if args.stdin:
        input_data = json.loads(sys.stdin.read())
    elif args.input:
        with open(args.input, 'r', encoding='utf-8') as f:
            input_data = json.load(f)
    else:
        parser.error("è¯·æŒ‡å®š --input æˆ– --stdin")
        return

    # æ”¯æŒä¸¤ç§è¾“å…¥æ ¼å¼ï¼šç›´æ¥æ•°ç»„æˆ– {"texts": [...]}
    if isinstance(input_data, list):
        texts = input_data
    else:
        texts = input_data.get('texts', [])

    # å¤„ç†
    try:
        results = process_texts(
            texts,
            eps=args.eps,
            min_samples=args.min_samples,
            min_length=args.min_length,
            auto_optimize=args.auto_optimize
        )

        output = {
            'success': True,
            'clusters': results,
            'total_clusters': len(results),
            'total_texts': sum(c['size'] for c in results)
        }
    except Exception as e:
        logger.error(f"å¤„ç†å¤±è´¥: {e}")
        output = {
            'success': False,
            'error': str(e),
            'clusters': []
        }

    # è¾“å‡ºç»“æœ
    result_json = json.dumps(output, ensure_ascii=False, indent=2)

    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(result_json)
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ° {args.output}")
    else:
        print(result_json)


if __name__ == '__main__':
    main()
